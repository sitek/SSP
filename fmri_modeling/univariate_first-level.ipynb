{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c080696c-3da0-42db-9137-99072ada6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from glob import glob\n",
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b7cf5-4dfa-4d67-8e1b-13608ce60660",
   "metadata": {},
   "source": [
    "## nilearn modeling: first level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15564ab-88b4-4d28-9938-3664b221200d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# based on: https://nilearn.github.io/auto_examples/04_glm_first_level/plot_bids_features.html#sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py\n",
    "\n",
    "def prep_models_and_args(subject_id=None, \n",
    "                         task_label=None, \n",
    "                         fwhm=None, \n",
    "                         bidsroot=None, \n",
    "                         deriv_dir=None, \n",
    "                         event_type=None, \n",
    "                         t_r=None, t_acq=None, \n",
    "                         space_label='MNI152NLin2009cAsym'):\n",
    "    from nilearn.glm.first_level import first_level_from_bids\n",
    "    from nilearn.interfaces.fmriprep import load_confounds\n",
    "\n",
    "    # correct the fmriprep-given slice reference (middle slice, or 0.5)\n",
    "    slice_time_ref = 0.5 * t_acq / t_r\n",
    "\n",
    "    print(bidsroot, task_label, space_label)\n",
    "\n",
    "    models, models_run_imgs, \\\n",
    "            models_events, \\\n",
    "            models_confounds = first_level_from_bids(bidsroot, \n",
    "                                                     task_label, \n",
    "                                                     space_label,\n",
    "                                                     [subject_id],\n",
    "                                                     smoothing_fwhm=fwhm,\n",
    "                                                     derivatives_folder=deriv_dir,\n",
    "                                                     slice_time_ref=slice_time_ref,\n",
    "                                                     minimize_memory=False)\n",
    "\n",
    "\n",
    "    ''' create events '''\n",
    "    # stimulus events\n",
    "    if event_type == 'stimulus':\n",
    "        for sx, sub_events in enumerate(models_events):\n",
    "            for mx, run_events in enumerate(sub_events):\n",
    "\n",
    "                name_groups = run_events.groupby('trial_type')['trial_type']\n",
    "                suffix = name_groups.cumcount() + 1\n",
    "                repeats = name_groups.transform('size')\n",
    "\n",
    "                run_events['trial_type'] = run_events['trial_type']\n",
    "                run_events['trial_type'] = run_events['trial_type'].str.replace('-','_')\n",
    "\n",
    "        # create stimulus list from updated events.tsv file\n",
    "        stim_list = sorted([s for s in run_events['trial_type'].unique() if str(s) != 'nan'])\n",
    "    \n",
    "    # trial-specific events\n",
    "    if event_type == 'trial':\n",
    "        for sx, sub_events in enumerate(models_events):\n",
    "            for mx, run_events in enumerate(sub_events):\n",
    "\n",
    "                name_groups = run_events.groupby('trial_type')['trial_type']\n",
    "                suffix = name_groups.cumcount() + 1\n",
    "                repeats = name_groups.transform('size')\n",
    "\n",
    "                run_events['trial_type'] = run_events['trial_type'] + \\\n",
    "                                                    '_trial' + suffix.map(str)\n",
    "                run_events['trial_type'] = run_events['trial_type'].str.replace('-','_')\n",
    "\n",
    "        # create stimulus list from updated events.tsv file\n",
    "        stim_list = sorted([s for s in run_events['trial_type'].unique() if str(s) != 'nan'])\n",
    "\n",
    "    # all sound events\n",
    "    elif event_type == 'sound':\n",
    "        for sx, sub_events in enumerate(models_events):\n",
    "            for mx, run_events in enumerate(sub_events):\n",
    "                orig_stim_list = sorted([str(s) for s in run_events['trial_type'].unique() if str(s) not in ['nan', 'None']])\n",
    "                #print('original stim list: ', orig_stim_list)\n",
    "\n",
    "                run_events['trial_type'] = run_events.trial_type.str.split('_', expand=True)[0]\n",
    "\n",
    "        # create stimulus list from updated events.tsv file\n",
    "        stim_list = sorted([str(s) for s in run_events['trial_type'].unique() if str(s) not in ['nan', 'None']])\n",
    "        #print('stim list: ', stim_list)\n",
    "\n",
    "    #model_and_args = zip(models, models_run_imgs, models_events, models_confounds)\n",
    "    return stim_list, models, models_run_imgs, models_events, models_confounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db69d270-cf4d-495b-92d0-690f88127492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Across-runs GLM\n",
    "def nilearn_glm_across_runs(stim_list, task_label, \n",
    "                            models, models_run_imgs, \n",
    "                            models_events, models_confounds, \n",
    "                            space_label):\n",
    "    from nilearn.reporting import make_glm_report\n",
    "    from nilearn.interfaces.bids import save_glm_to_bids\n",
    "    from nilearn.interfaces.fmriprep import load_confounds_strategy\n",
    "    \n",
    "    for midx in range(len(models)):\n",
    "        #for sx, stim in enumerate(stim_list):\n",
    "        \n",
    "        for contrast_label in ['sound', 'response']:\n",
    "            #contrast_desc  = stim\n",
    "            print('Running for contrast', contrast_label)\n",
    "            \n",
    "            midx = 0 # only 1 subject per analysis\n",
    "            model = models[midx]\n",
    "            imgs = models_run_imgs[midx]\n",
    "            events = models_events[midx]\n",
    "            #confounds = models_confounds[midx]\n",
    "\n",
    "            # set limited confounds\n",
    "            print('selecting confounds')\n",
    "            confounds_ltd, sample_mask = load_confounds_strategy(img_files=imgs, \n",
    "                                                                 denoise_strategy='compcor')\n",
    "            \n",
    "            try:\n",
    "                # fit the GLM\n",
    "                print('fitting GLM')\n",
    "                model.fit(imgs, events, confounds_ltd); \n",
    "                print(model)\n",
    "\n",
    "                # compute the contrast of interest\n",
    "                print('computing contrast of interest')\n",
    "                summary_statistics = model.compute_contrast(contrast_label, output_type='all')\n",
    "                zmap = summary_statistics['z_score']\n",
    "                tmap = summary_statistics['stat']\n",
    "                statmap = summary_statistics['effect_size']\n",
    "\n",
    "                ''' ADD BIDS OUTPUTS '''\n",
    "                bidsderiv_dir = os.path.join(bidsroot, 'derivatives', \n",
    "                                             'nilearn', 'run-all')\n",
    "                if not os.path.exists(bidsderiv_dir):\n",
    "                    os.makedirs(bidsderiv_dir)\n",
    "\n",
    "                out_prefix = f\"sub-{model.subject_label}_task-{task_label}_fwhm-{model.smoothing_fwhm}\"\n",
    "                save_glm_to_bids(model, \n",
    "                                 contrast_label,\n",
    "                                 out_dir=bidsderiv_dir,\n",
    "                                 prefix=out_prefix,\n",
    "                                )\n",
    "                print(f'Saved model outputs to {bidsderiv_dir}')\n",
    "\n",
    "            except:\n",
    "                print('could not run for ', contrast_label)\n",
    "        return confounds_ltd#summary_statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9c16c-089d-4d5f-8f7f-cacea48e1a89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838b8d03-b604-4f76-8ca9-48d82b833f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidsroot:  /bgfs/bchandrasekaran/krs228/data/SSP/data_bids\n",
      "fmriprep dir: /bgfs/bchandrasekaran/krs228/data/SSP/data_bids/derivatives/fmriprep-23.2.1\n"
     ]
    }
   ],
   "source": [
    "task_label = 'badaga'\n",
    "t_acq = 2\n",
    "t_r = t_acq # no silent gap in this acquisition\n",
    "\n",
    "project_dir = os.path.join('/bgfs/bchandrasekaran/krs228/data/', \n",
    "                           'SSP/')\n",
    "bidsroot = os.path.join(project_dir, \n",
    "                        'data_bids')\n",
    "fmriprep_dir = os.path.join(bidsroot, \n",
    "                            'derivatives', \n",
    "                            'fmriprep-23.2.1',\n",
    "                            )\n",
    "print('bidsroot: ', bidsroot)\n",
    "print('fmriprep dir:', fmriprep_dir)\n",
    "\n",
    "nilearn_dir = os.path.join(bidsroot, 'derivatives', 'nilearn')\n",
    "if not os.path.exists(nilearn_dir):\n",
    "        os.makedirs(nilearn_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818abfcb-2347-4ac1-b780-97744b191642",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c5b7f-712b-4d09-8530-fe52eece6a3e",
   "metadata": {},
   "source": [
    "Takes approximately 30 minutes per subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2e475-5410-4460-9548-a5b459c2a553",
   "metadata": {},
   "source": [
    "**from Ashley Feb 3, 2025:**\n",
    "\n",
    "So, please IGNORE for now:\n",
    "- 001\n",
    "- 002\n",
    "- 005\n",
    "- 014\n",
    "- 072\n",
    "- (and maybe 069?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9059cc-69c2-42eb-a9c6-10aafccff5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwhm = 6\n",
    "space_label = 'MNI152NLin2009cAsym'\n",
    "subject_list = [#'SSP009', 'SSP011', \n",
    "                #'SSP012', 'SSP013', # only 2 beh events files ?\n",
    "                #'SSP017', 'SSP018',\n",
    "                #'SSP020', 'SSP028', 'SSP032', 'SSP033',\n",
    "                #'SSP034', 'SSP036', 'SSP038', 'SSP039',\n",
    "                #'SSP040', \n",
    "                #'SSP041', 'SSP038', 'SSP045',\n",
    "                #'SSP046', 'SSP048', 'SSP051', 'SSP054',\n",
    "                #'SSP058', 'SSP059', 'SSP069',\n",
    "                ] # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34169573-4ce7-4cda-a7b0-203c8e433cdb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sx, subject_id in enumerate(subject_list):\n",
    "    print('Running subject ', subject_id)\n",
    "    stim_list, models, \\\n",
    "               models_run_imgs, \\\n",
    "               models_events, \\\n",
    "               models_confounds = prep_models_and_args(subject_id, task_label, \n",
    "                                                     fwhm, bidsroot, \n",
    "                                                     fmriprep_dir, 'sound',\n",
    "                                                     t_r, t_acq, \n",
    "                                                     space_label)\n",
    "   \n",
    "    # Across-run GLM\n",
    "    confounds_ltd = nilearn_glm_across_runs(stim_list, task_label, \n",
    "                                             models, models_run_imgs, \n",
    "                                             models_events, models_confounds, \n",
    "                                             space_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec2826-442e-4985-9044-bfb62c4eb4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
